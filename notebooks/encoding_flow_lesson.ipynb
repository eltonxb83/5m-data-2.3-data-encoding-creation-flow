{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Encoding, Decoding and Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Updated (27 March 2025):\n",
    "> This notebook has been split into 4 parts for ease of reference:\n",
    ">\n",
    "> 1. Apache Thrift - [`demo_thrift.ipynb`](notebooks/demo_thrift.ipynb)\n",
    "> 2. Protocol Buffers (Protobufs) - [`demo_protobuf.ipynb`](notebooks/demo_protobuf.ipynb)\n",
    "> 3. Apache Avro - [`demo_avro.ipynb`](notebooks/demo_avro.ipynb)\n",
    "> 4. Apache Parquet, ORC and Arrow - [`demo_parquet_orc_arrow.ipynb`](notebooks/demo_parquet_orc_arrow.ipynb)\n",
    ">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Thrift\n",
    "\n",
    "The thrift type system includes base types like _bool, byte, double, string and integer_ but also special types like _binary_ and _struct_ (like classes) and also containers (_list, set, map_) that correspond to commonly available interfaces in most programming languages.\n",
    "\n",
    "Base types:\n",
    "\n",
    "- bool: A boolean value (true or false)\n",
    "- byte: An 8-bit signed integer\n",
    "- i16: A 16-bit signed integer\n",
    "- i32: A 32-bit signed integer\n",
    "- i64: A 64-bit signed integer\n",
    "- double: A 64-bit floating point number\n",
    "- string: A text string encoded using UTF-8 encoding\n",
    "\n",
    "Thrift type definitions are defined in `.thrift` files. The Thrift compiler generates code in various languages from the `.thrift` files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "\n",
    "Let's use the following example record (JSON or dictionary-like) to encode:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"userName\": \"Martin\",\n",
    "  \"favoriteNumber\": 1337,\n",
    "  \"interests\": [\"daydreaming\", \"hacking\"]\n",
    "}\n",
    "```\n",
    "\n",
    "We can encode the record in Thrift using the following schema in the `.thrift` file:\n",
    "\n",
    "```thrift\n",
    "struct Person {\n",
    "  1: required string userName,\n",
    "  2: optional i64 favoriteNumber,\n",
    "  3: optional list<string> interests\n",
    "}\n",
    "```\n",
    "\n",
    "Thrift comes with a code generation tool that takes a schema definition like the ones shown here, and produces classes that implement the schema in various programming languages. Our code can call this generated code to encode or decode records of the schema.\n",
    "\n",
    "The data encoded with this schema looks like this:\n",
    "![thrift_binary_protocol](../assets/thrift_binary_protocol.png)\n",
    "\n",
    "Each field has a type annotation (to indicate whether it is a string, integer, list, etc.) and, where required, a length indication (length of a string, number of items in a list). The strings that appear in the data (“Martin”, “daydreaming”, “hacking”) are encoded as UTF-8.\n",
    "\n",
    "There are no field names (userName, favoriteNumber, interests). Instead, the encoded data contains _field tags_, which are numbers (1, 2, and 3). Those are the numbers that appear in the schema definition. Field tags are like aliases for fields—they are a compact way of saying what field we’re talking about, without having to spell out the field name.\n",
    "\n",
    "Next, let's add a service. A service is a collection of method interfaces that can be called remotely. A service is defined in a `.thrift` file like this:\n",
    "\n",
    "```thrift\n",
    "service School {\n",
    "    Person teachCourse(1: required Person person, 2: required string course)\n",
    "}\n",
    "```\n",
    "\n",
    "The first line declares a service called `School`. The second line declares a method called `teachCourses`, which takes two arguments: a `Person` record and a `string`. The method returns a `Person` record.\n",
    "\n",
    "### RPC\n",
    "\n",
    "Nows, let's look at how to use the generated code to make remote procedure calls. We will write codes for 2 sides of the server-client application- the client initiates an RPC call and waits for a response from the server. The server executes the requested operation and returns a response to the client.\n",
    "\n",
    "Here, we use `%%writefile` magic command to write the code to a file instead of running it in the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../schema/person.thrift\n",
    "\n",
    "struct Person {\n",
    "  1: required string userName,\n",
    "  2: optional i64 favoriteNumber,\n",
    "  3: optional list<string> interests\n",
    "}\n",
    "\n",
    "service School {\n",
    "    Person teachCourse(1: required Person person, 2: required string course)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../person_thrift_server.py\n",
    "import thriftpy2\n",
    "person_thrift = thriftpy2.load(\"./schema/person.thrift\", module_name=\"person_thrift\")\n",
    "\n",
    "from thriftpy2.rpc import make_server\n",
    "\n",
    "class School(object):\n",
    "    def teachCourse(self, person, course):\n",
    "        person.interests.append(course)\n",
    "        return person\n",
    "\n",
    "server = make_server(person_thrift.School, School(), client_timeout=None)\n",
    "server.serve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, run `python person_thrift_server.py` in a new terminal. This will start the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import thriftpy2\n",
    "person_thrift = thriftpy2.load(\"../schema/person.thrift\", module_name=\"person_thrift\")\n",
    "\n",
    "from thriftpy2.rpc import make_client\n",
    "\n",
    "school = make_client(person_thrift.School, timeout=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "martin = person_thrift.Person(\n",
    "    userName=\"Martin\", favoriteNumber=1337, interests=[\"daydreaming\", \"hacking\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "martin.interests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "martin = school.teachCourse(martin, \"coding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "martin.interests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Add a new field `grade` (0-100) with an appropriate type annotation to the `Person` struct. Then, add a new method `assignGrade` to the `School` service that takes a `Person` record and a `grade` arguments, assigns the `grade` to the `Person` and returns the `Person`. Then call the method by passing `martin` and a grade number, and print his grade.\n",
    ">\n",
    "> 2. Add a method `teachCourses` to School to add a list of courses instead of just one course. Then pass `martin` and a list of course-- `[\"cooking\", \"sewing\"]` to the method, and print his new interests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Protocol Buffers (Protobuf)\n",
    "\n",
    "Protobuf types include:\n",
    "\n",
    "- double: double precision floating point number\n",
    "- float: single precision floating point number\n",
    "- int32: signed integer, uses variable-length encoding\n",
    "- int64: signed integer, uses variable-length encoding\n",
    "- uint32: unsigned integer, uses variable-length encoding\n",
    "- uint64: unsigned integer, uses variable-length encoding\n",
    "- sint32: signed integer, uses variable-length encoding, more efficient than int32\n",
    "- sint64: signed integer, uses variable-length encoding, more efficient than int64\n",
    "- fixed32: unsigned integer, always 4 bytes\n",
    "- fixed64: unsigned integer, always 8 bytes\n",
    "- sfixed32: signed integer, always 4 bytes\n",
    "- sfixed64: signed integer, always 8 bytes\n",
    "- bool: boolean value\n",
    "- string: UTF-8 text string\n",
    "- bytes: sequence of bytes\n",
    "- enum: enumerated type\n",
    "- message: nested message type\n",
    "- map: map type\n",
    "- Any: dynamic type\n",
    "\n",
    "Protobuf schema definitions are defined in `.proto` files. The Protobuf compiler generates code in various languages from the `.proto` files.\n",
    "\n",
    "### Encoding\n",
    "\n",
    "We can encode the previous example record in Protobuf using the following schema in the `.proto` file:\n",
    "\n",
    "```protobuf\n",
    "message Person {\n",
    "  required string user_name = 1;\n",
    "  optional int64 favorite_number = 2;\n",
    "  repeated string interests = 3;\n",
    "}\n",
    "```\n",
    "\n",
    "The data encoded with this schema looks like this:\n",
    "![protobuf](../assets/protobuf.png)\n",
    "\n",
    "Protocol Buffers have an interesting aspect regarding its datatype handling. Unlike having a specific list or array datatype, it utilizes a `repeated` marker for fields, which serves as a third option alongside `required` and `optional`.\n",
    "\n",
    "As depicted in the figure, a repeated field is simply represented by the same field tag appearing multiple times in the record. The advantage of this approach is that converting an optional (single-valued) field into a repeated (multi-valued) field is permissible. When new code reads old data, it interprets it as a list with either zero or one element, depending on whether the field was present. On the other hand, old code reading new data only perceives the last element of the list.\n",
    "\n",
    "### gRPC\n",
    "\n",
    "Nows, let's look at how to use the generated code to make remote procedure calls. We will use gRPC, which is a high-performance RPC framework built on top of Protobuf. gRPC is a client-server application where the client initiates an RPC call and waits for a response from the server. The server executes the requested operation and returns a response to the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../schema/person.proto\n",
    "syntax = \"proto3\";\n",
    "\n",
    "message Person {\n",
    "  string user_name = 1;\n",
    "  optional int64 favorite_number = 2;\n",
    "  repeated string interests = 3;\n",
    "}\n",
    "\n",
    "message CourseRequest {\n",
    "  Person person = 1;\n",
    "  string course = 2;\n",
    "}\n",
    "\n",
    "\n",
    "service School {\n",
    "  rpc teachCourse(CourseRequest) returns (Person) {}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, run the following command in a terminal to generate the Python code:\n",
    "\n",
    "```bash\n",
    "python -m grpc_tools.protoc -I./schema --python_out=. --grpc_python_out=. ./schema/person.proto\n",
    "```\n",
    "\n",
    "This will generate the following files:\n",
    "\n",
    "```bash\n",
    "person_pb2.py\n",
    "person_pb2_grpc.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../person_protobuf_server.py\n",
    "from concurrent import futures\n",
    "import grpc\n",
    "import person_pb2_grpc\n",
    "\n",
    "\n",
    "class School(person_pb2_grpc.SchoolServicer):\n",
    "\n",
    "  def teachCourse(self, request, context):\n",
    "    request.person.interests.append(request.course)\n",
    "    return request.person\n",
    "\n",
    "server = grpc.server(futures.ThreadPoolExecutor(max_workers=2))\n",
    "person_pb2_grpc.add_SchoolServicer_to_server(\n",
    "    School(), server)\n",
    "server.add_insecure_port('[::]:50051')\n",
    "server.start()\n",
    "server.wait_for_termination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, run `python person_protobuf_server.py` in a new terminal. This will start the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import grpc\n",
    "import person_pb2\n",
    "import person_pb2_grpc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def teach_course(stub, person, course):\n",
    "    person = stub.teachCourse(person_pb2.CourseRequest(person=person, course=course))\n",
    "    return person\n",
    "\n",
    "\n",
    "with grpc.insecure_channel('localhost:50051') as channel:\n",
    "    martin = person_pb2.Person(user_name='Martin', favorite_number=1337, interests=[\"daydreaming\", \"hacking\"])\n",
    "    course = \"coding\"\n",
    "    stub = person_pb2_grpc.SchoolStub(channel)\n",
    "    martin = teach_course(stub, martin, course)\n",
    "    print(martin.interests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Add a new field `grade` (0-100) with an appropriate type annotation to the `Person` message. Then, add a new method `assignGrade` to the `School` service that takes a `GradeRequest` message (which consists of a `Person` record and a `grade` arguments), assigns the `grade` to the `Person` and returns the `Person`. Then call the method by passing \"Martin\" `person` and a grade number, and print his grade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Schema evolution** is a key concept in **data serialization and storage formats** like **Avro, Parquet, ORC, Protobuf, and Thrift**. \n",
    "\n",
    "---\n",
    "\n",
    "# **1. What is schema evolution?**\n",
    "\n",
    "**Schema evolution** is the ability of a data system to **handle changes to the schema** of your data **without breaking existing data or applications**.\n",
    "\n",
    "* Schema = the structure of your data (fields, types, names).\n",
    "* Evolution = **adding/removing/modifying fields over time**.\n",
    "\n",
    "In other words:\n",
    "\n",
    "> You can change your data format and still read old data, or write new data while old readers still work.\n",
    "\n",
    "---\n",
    "\n",
    "# **2. Why schema evolution matters**\n",
    "\n",
    "In real-world systems:\n",
    "\n",
    "* Data pipelines are long-lived.\n",
    "* Requirements change over time.\n",
    "* You may add new columns, remove old ones, or rename them.\n",
    "\n",
    "Without schema evolution, **changing a schema would break all old data** or require **rewriting everything**.\n",
    "\n",
    "---\n",
    "\n",
    "# **3. How different systems support it**\n",
    "\n",
    "### **Avro (very strong support)**\n",
    "\n",
    "* Every Avro file contains its **writer schema**.\n",
    "* When you read it, you can provide a **reader schema**.\n",
    "* Supports:\n",
    "\n",
    "| Change type    | Allowed?       | How it works                         |\n",
    "| -------------- | -------------- | ------------------------------------ |\n",
    "| Add a field    | ✅              | Provide a default value for old data |\n",
    "| Remove a field | ✅              | Old readers ignore extra fields      |\n",
    "| Rename a field | ✅ (with alias) | Use `\"aliases\": [\"old_name\"]`        |\n",
    "| Change type    | ⚠              | Only compatible types (int → long)   |\n",
    "\n",
    "Example:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"type\": \"record\",\n",
    "  \"name\": \"User\",\n",
    "  \"fields\": [\n",
    "    {\"name\": \"name\", \"type\": \"string\"},\n",
    "    {\"name\": \"age\", \"type\": [\"null\", \"int\"], \"default\": null},\n",
    "    {\"name\": \"email\", \"type\": \"string\", \"default\": \"\"}\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "* Adding `email` later is safe if a default is provided.\n",
    "\n",
    "---\n",
    "\n",
    "### **Protobuf**\n",
    "\n",
    "* Supports **adding new fields** and **ignoring unknown fields**.\n",
    "* Each field has a **unique tag number**.\n",
    "* Rules:\n",
    "\n",
    "1. Add new fields → OK\n",
    "2. Remove fields → OK, but don’t reuse tag numbers\n",
    "3. Change types → only if backward compatible\n",
    "\n",
    "---\n",
    "\n",
    "### **Thrift**\n",
    "\n",
    "* Similar to Protobuf.\n",
    "* Uses **field IDs** for compatibility.\n",
    "* Optional vs required fields are critical.\n",
    "\n",
    "---\n",
    "\n",
    "### **Parquet / ORC**\n",
    "\n",
    "* Schema evolution is supported **column-wise**.\n",
    "* Adding new columns → OK\n",
    "* Removing columns → OK\n",
    "* Type changes → sometimes tricky; need compatible types\n",
    "\n",
    "---\n",
    "\n",
    "# **4. Real-world analogy**\n",
    "\n",
    "Imagine a database table:\n",
    "\n",
    "| name | age |\n",
    "\n",
    "Later you want to add `email` and remove `age`:\n",
    "\n",
    "* **With schema evolution:**\n",
    "\n",
    "  * Old rows without `email` → default value\n",
    "  * Applications reading old files → still work\n",
    "\n",
    "* **Without schema evolution:**\n",
    "\n",
    "  * Old data is broken or unreadable\n",
    "\n",
    "---\n",
    "\n",
    "# **5. Key points**\n",
    "\n",
    "* Schema evolution is **mostly backward and forward compatible**.\n",
    "* Essential for **streaming systems** (Kafka + Avro), **data lakes** (Parquet, ORC).\n",
    "* Works best when **default values and optional fields** are used.\n",
    "* Helps **avoid rewriting or migrating massive datasets** when the schema changes.\n",
    "\n",
    "---\n",
    "\n",
    "# **6. Visual summary**\n",
    "\n",
    "```\n",
    "Old schema:        name | age\n",
    "New schema:        name | age | email\n",
    "Backward compatible: old readers still read name & age, ignore email\n",
    "Forward compatible: new readers can provide default for missing email in old data\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema Evolution for Thrift and Protobuf\n",
    "\n",
    "Let's explore how Thrift and Protocol Buffers manage schema changes to ensure both _backward_ and _forward compatibility_.\n",
    "\n",
    "From the examples above, an encoded record comprises concatenated encoded fields. Each field is identified by a tag number (e.g., 1, 2, 3 in the sample schemas) and annotated with a datatype (such as string or integer). If a field value is not set, it is simply omitted from the encoded record. It's important to note that field tags play a crucial role in determining the meaning of the encoded data. Although you can modify a field's name in the schema without issues, changing a field's tag would render all existing encoded data invalid.\n",
    "\n",
    "Adding new fields to the schema is permissible, but it requires assigning each field a new tag number. Old code, which is unaware of the new tag numbers, can still read data written by new code, and if it encounters an unrecognized tag number, it can safely ignore that field. The datatype annotation allows the parser to determine the necessary bytes to skip, thus maintaining _forward compatibility_.\n",
    "\n",
    "Regarding _backward compatibility_, as long as each field retains a unique tag number, new code can read old data successfully because the tag numbers retain their original meaning. However, when adding a new field, it cannot be made required since old code wouldn't have written this new field, causing compatibility issues. Hence, to ensure backward compatibility, **any fields added after the initial schema deployment must be optional or have default values**.\n",
    "\n",
    "Removing a field follows a similar principle to adding one, but with backward and forward compatibility concerns reversed. Only optional fields can be removed, never required fields, and once a tag number is removed, it can't be reused to prevent conflicts with existing data written with the old tag number.\n",
    "\n",
    "Changing the datatype of a field is possible, but it carries the risk of losing precision or truncating values. For instance, converting a 32-bit integer into a 64-bit integer may lead to data loss when old code reads data written by new code, as the old code uses a 32-bit variable to hold the value. If the 64-bit value exceeds the capacity of a 32-bit variable, it will be truncated. Thus, careful consideration is necessary when altering field datatypes, and consulting the documentation is advisable to understand potential implications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Avro\n",
    "\n",
    "Avro has the following types:\n",
    "\n",
    "- null: no value\n",
    "- boolean: a binary value\n",
    "- int: 32-bit signed integer\n",
    "- long: 64-bit signed integer\n",
    "- float: single precision (32-bit) IEEE 754 floating-point number\n",
    "- double: double precision (64-bit) IEEE 754 floating-point number\n",
    "- bytes: sequence of 8-bit unsigned bytes\n",
    "- string: Unicode character sequence\n",
    "- record: ordered collection of named fields\n",
    "- enum: enumeration of string values\n",
    "- array: ordered collection of values\n",
    "- map: collection of key-value pairs\n",
    "- union: ordered list of values\n",
    "\n",
    "It has two schema languages: one (`Avro IDL`) intended for human editing, and one (based on JSON) that is more easily machine-readable.\n",
    "\n",
    "### Encoding\n",
    "\n",
    "We can encode the previous example record in IDL using the following schema in the `.avsc` file:\n",
    "\n",
    "```avro\n",
    "record Person {\n",
    "  string userName;\n",
    "  union { null, long } favoriteNumber = null;\n",
    "  array<string> interests;\n",
    "}\n",
    "```\n",
    "\n",
    "The equivalent JSON representation of that schema is as follows:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"type\": \"record\",\n",
    "  \"name\": \"Person\",\n",
    "  \"fields\": [\n",
    "    { \"name\": \"userName\", \"type\": \"string\" },\n",
    "    { \"name\": \"favoriteNumber\", \"type\": [\"null\", \"long\"], \"default\": null },\n",
    "    { \"name\": \"interests\", \"type\": { \"type\": \"array\", \"items\": \"string\" } }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "The data encoded with this schema looks like this:\n",
    "![avro](../assets/avro.png)\n",
    "\n",
    "First and foremost, it's important to note that the schema lacks tag numbers. When we encode our sample record using this schema, the resulting Avro binary encoding is impressively compact, spanning just _32 bytes_—the most space-efficient among all the encodings we've observed.\n",
    "\n",
    "Examining the byte sequence, one can readily discern the _absence of field identifiers or datatype markers_. The encoding solely comprises concatenated values. For instance, a string is represented by a length prefix followed by UTF-8 bytes, but there are no explicit indicators within the encoded data to specify that it is, indeed, a string. In fact, it could be interpreted as an integer or any other data type altogether. Similarly, an integer is encoded using a variable-length encoding.\n",
    "\n",
    "To correctly parse the binary data, you must traverse the fields in the order they appear in the schema and _refer to the schema_ itself to ascertain the datatype of each field. Consequently, the binary data can only be accurately decoded if the code reading the data employs the exact same schema as the code that wrote the data. Any deviation or mismatch in the schema between the reader and the writer would result in incorrectly decoded data.\n",
    "\n",
    "With Avro, data encoding and decoding are based on two schemas: the `writer's schema` used during data encoding and the `reader's schema` employed during data decoding. These schemas do not necessarily have to be identical but should be compatible. When decoding data, the Avro library compares the writer's and reader's schemas, resolving any discrepancies between them.\n",
    "\n",
    "The Avro specification ensures that fields in different orders between the writer's and reader's schemas pose no issues during resolution since schema matching occurs based on field names. If the reader's schema lacks a field present in the writer's schema, it is simply ignored. Conversely, if the reader's schema expects a field that the writer's schema does not contain, the missing field is filled in with a default value declared in the reader's schema. This allows for flexible schema evolution while maintaining data compatibility.\n",
    "\n",
    "### Reading (Decoding) a File\n",
    "\n",
    "Instead of demonstrating RPC, let's look at how to decode data from a file from a real-world dataset. We have a genomic variation data of 1000 samples from the [OpenCGA](http://docs.opencb.org/display/opencga/Welcome+to+OpenCGA) project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastavro\n",
    "import copy\n",
    "import json\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/1k.variants.avro', 'rb') as f:\n",
    "    reader = fastavro.reader(f)\n",
    "    genomic_var_1k = [sample for sample in reader]\n",
    "    metadata = copy.deepcopy(reader.metadata)\n",
    "    writer_schema = copy.deepcopy(reader.writer_schema)\n",
    "    schema_from_file = json.loads(metadata['avro.schema'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(genomic_var_1k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(writer_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(schema_from_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(genomic_var_1k[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in schema_from_file[\"fields\"]:\n",
    "    print(f[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Parquet, ORC and Arrow\n",
    "\n",
    "We can easily read (decode) and write (encode) data from and to Parquet, ORC and Arrow files interchangeably. The `pyarrow` library allows us to read a Parquet or ORC file into a `pyarrow.Table` object, which is a columnar data structure that can be converted to a Pandas DataFrame. We can also write a `pyarrow.Table` to a Parquet or ORC file.\n",
    "\n",
    "Parquet has the following types:\n",
    "\n",
    "- boolean: 1 bit boolean\n",
    "- int32: 32 bit signed ints\n",
    "- int64: 64 bit signed ints\n",
    "- int96: 96 bit signed ints\n",
    "- float: IEEE 32-bit floating point values\n",
    "- double: IEEE 64-bit floating point values\n",
    "- byte_array: arbitrarily long byte arrays\n",
    "- fixed_len_byte_array: fixed length byte arrays\n",
    "- string: UTF-8 encoded strings\n",
    "- enum: enumeration of strings\n",
    "- temporal: a logical date type\n",
    "\n",
    "ORC has the following types:\n",
    "\n",
    "- boolean: 1 bit boolean\n",
    "- tinyint: 8 bit signed ints\n",
    "- smallint: 16 bit signed ints\n",
    "- int: 32 bit signed ints\n",
    "- bigint: 64 bit signed ints\n",
    "- float: IEEE 32-bit floating point values\n",
    "- double: IEEE 64-bit floating point values\n",
    "- string: UTF-8 encoded strings\n",
    "- char: ASCII strings\n",
    "- varchar: UTF-8 strings\n",
    "- binary: byte arrays\n",
    "- timestamp: a logical date type\n",
    "- date: a logical date type\n",
    "- decimal: arbitrary precision decimals\n",
    "- list: an ordered collection of objects\n",
    "- map: a collection of key-value pairs\n",
    "- struct: an ordered collection of named fields\n",
    "- union: a list of types\n",
    "\n",
    "### Reading (Decoding) and Writing (Encoding) a Parquet File\n",
    "\n",
    "Let's look at how to decode and encode a Parquet file with mock customers data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pq.read_table('../data/userdata1.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pq.read_metadata('../data/userdata1.parquet')\n",
    "\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.row_group(0).column(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the first 3 rows of the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.take([0,1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert a Table to a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = table.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can convert the DataFrame back to a Table (note we're using the method from `pa` which is pyarrow):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_table = pa.Table.from_pandas(df)\n",
    "\n",
    "new_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can write the table back to a Parquet file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq.write_table(new_table, \"../data/userdata2.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. How many males and females are there?\n",
    ">\n",
    "> 2. What is the average salary for customers from China?\n",
    ">\n",
    "> 3. Create a new column `full_name` which combines `first_name` and `last_name` with a space in between in the dataframe. Then convert it back to a new Table and write it to a Parquet file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading (Decoding) and Writing (Encoding) an ORC File\n",
    "\n",
    "Let's look at how to decode and encode an ORC file with mock customers data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "from pyarrow import orc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table2 = orc.read_table('../data/userdata1.1.orc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column names are missing in the ORC file, so we need to specify them manually, we can use the column names from the previous Table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table2 = table2.rename_columns(table.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = table2.to_pandas()\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can write the table back to an ORC file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orc.write_table(table2, \"../data/userdata2.orc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. How many males and females are there from China?\n",
    ">\n",
    "> 2. Create a new column `age` which is computed from the birthdate in the dataframe. Then convert it back to a new Table and write it to an ORC file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bde]",
   "language": "python",
   "name": "conda-env-bde-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
