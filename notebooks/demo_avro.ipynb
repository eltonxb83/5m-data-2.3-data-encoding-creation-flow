{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Encoding, Decoding and Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ‚úÖ **What is Apache Avro?**\n",
    "\n",
    "**Apache Avro** is a data serialization system created under the Apache Hadoop project.\n",
    "It‚Äôs designed for:\n",
    "\n",
    "* **Big Data systems (Hadoop, Kafka, streaming pipelines)**\n",
    "* **Dynamic schemas**\n",
    "* **Schema evolution**\n",
    "* **Fast, compact binary data**\n",
    "\n",
    "Avro is *similar* in purpose to Protocol Buffers and Thrift‚Ä¶\n",
    "but the **design philosophy is very different**.\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£ **Key Features of Avro**\n",
    "\n",
    "## **‚úî Schema is JSON**\n",
    "\n",
    "Avro schemas are written in plain **JSON**, such as:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"type\": \"record\",\n",
    "  \"name\": \"Person\",\n",
    "  \"fields\": [\n",
    "    {\"name\": \"user_name\", \"type\": \"string\"},\n",
    "    {\"name\": \"favorite_number\", \"type\": [\"null\", \"long\"]},\n",
    "    {\"name\": \"interests\", \"type\": {\"type\": \"array\", \"items\": \"string\"}}\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "Unlike Protobuf and Thrift which use their own `.proto` / `.thrift` languages.\n",
    "\n",
    "---\n",
    "\n",
    "## **‚úî Self-Describing Data (Optional)**\n",
    "\n",
    "Avro allows embedding the **schema inside the data file** (e.g., in Avro container files).\n",
    "This is why it‚Äôs extremely popular in Hadoop / Kafka:\n",
    "\n",
    "* Producer writes the message\n",
    "* Consumer doesn't need the exact same class definitions\n",
    "* Schema is stored externally or inside the file\n",
    "\n",
    "---\n",
    "\n",
    "## **‚úî Dynamic Typing**\n",
    "\n",
    "You **do not need generated source code** if you don‚Äôt want it.\n",
    "You can read/write Avro purely using:\n",
    "\n",
    "* Dictionaries in Python\n",
    "* HashMaps in Java\n",
    "* GenericRecord types in JVM languages\n",
    "\n",
    "vs Protobuf/Thrift which *require* generated classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **‚úî Great for Schema Evolution**\n",
    "\n",
    "Avro has one of the strongest schema evolution systems:\n",
    "\n",
    "* Add fields\n",
    "* Remove fields\n",
    "* Change defaults\n",
    "* Make fields nullable\n",
    "\n",
    "Because Avro stores the data using **field names**, not numeric tags.\n",
    "\n",
    "Protobuf uses numeric tags ‚Üí faster but stricter.\n",
    "\n",
    "---\n",
    "\n",
    "## **‚úî Designed for Big Data**\n",
    "\n",
    "Avro is the native serialization system used in:\n",
    "\n",
    "* **Kafka**\n",
    "* **Hadoop / HDFS**\n",
    "* **Spark**\n",
    "* **Hive**\n",
    "* **Flink**\n",
    "\n",
    "Its container format supports:\n",
    "\n",
    "* Compression\n",
    "* Splittable files\n",
    "* Embedded schema\n",
    "* Ready for distributed processing\n",
    "\n",
    "This is why Avro dominates data engineering workloads.\n",
    "\n",
    "---\n",
    "\n",
    "# 2Ô∏è‚É£ Avro vs Protobuf vs Thrift\n",
    "\n",
    "Here‚Äôs the clearest way to compare:\n",
    "\n",
    "### **Serialization Purpose**\n",
    "\n",
    "| Feature      | Avro                            | Protobuf                          | Thrift                       |\n",
    "| ------------ | ------------------------------- | --------------------------------- | ---------------------------- |\n",
    "| Main Purpose | Data serialization (big data)   | Serialization + modern RPC (gRPC) | Serialization + built-in RPC |\n",
    "| Best For     | Kafka / Hadoop / Data pipelines | Microservices                     | Legacy / multi-protocol RPC  |\n",
    "\n",
    "---\n",
    "\n",
    "### **Schema Format**\n",
    "\n",
    "| Value                      | Avro     | Protobuf   | Thrift      |\n",
    "| -------------------------- | -------- | ---------- | ----------- |\n",
    "| Schema Language            | JSON     | .proto DSL | .thrift DSL |\n",
    "| Self-describing data       | Yes      | No         | No          |\n",
    "| Required generated classes | Optional | Required   | Required    |\n",
    "\n",
    "---\n",
    "\n",
    "### **Serialization Style**\n",
    "\n",
    "| Value                    | Avro                 | Protobuf       | Thrift         |\n",
    "| ------------------------ | -------------------- | -------------- | -------------- |\n",
    "| Encoding                 | Binary + JSON schema | Compact binary | Compact binary |\n",
    "| Uses field names or tags | Names                | Numeric tags   | Numeric tags   |\n",
    "| Speed                    | Fast                 | Fastest        | Fast           |\n",
    "\n",
    "Protobuf is typically the fastest, but Avro is close and has richer schema flexibility.\n",
    "\n",
    "---\n",
    "\n",
    "### **RPC Support**\n",
    "\n",
    "| Feature           | Avro                        | Protobuf         | Thrift              |\n",
    "| ----------------- | --------------------------- | ---------------- | ------------------- |\n",
    "| Built-in RPC      | Yes (Avro RPC, rarely used) | No (use gRPC)    | Yes (commonly used) |\n",
    "| Popular RPC usage | Very rare                   | Extremely common | Moderate            |\n",
    "\n",
    "Most people use Avro **without** its RPC system.\n",
    "\n",
    "---\n",
    "\n",
    "# 3Ô∏è‚É£ What Does a Typical Avro Workflow Look Like?\n",
    "\n",
    "## Client/Server flow in Avro is more like:\n",
    "\n",
    "1. You define a **JSON schema**\n",
    "2. You serialize data (Python dict ‚Üí Avro binary)\n",
    "3. You send that data via:\n",
    "\n",
    "   * Kafka\n",
    "   * Hadoop\n",
    "   * REST\n",
    "   * Anything you want\n",
    "4. The consumer reads the binary using:\n",
    "\n",
    "   * The writer schema (embedded or referenced)\n",
    "   * Its own reader schema\n",
    "\n",
    "No stubs, no service generators unless you use Avro RPC.\n",
    "\n",
    "This differs from:\n",
    "\n",
    "* **gRPC (Protobuf)** ‚Äî auto-generated client & server\n",
    "* **Thrift RPC** ‚Äî auto-generated client & server\n",
    "* **Avro** ‚Äî mostly just raw data serialization\n",
    "\n",
    "---\n",
    "\n",
    "# 4Ô∏è‚É£ **In Your Case (with gRPC / Protobuf / Thrift)**\n",
    "\n",
    "Your earlier code (Protobuf ‚Üí gRPC server ‚Üí client stub) looks like:\n",
    "\n",
    "```\n",
    "client ‚Üí stub ‚Üí gRPC ‚Üí protobuf ‚Üí server ‚Üí return protobuf\n",
    "```\n",
    "\n",
    "If you used Thrift RPC:\n",
    "\n",
    "```\n",
    "client ‚Üí thrift client ‚Üí thrift RPC ‚Üí server ‚Üí return thrift object\n",
    "```\n",
    "\n",
    "But with Avro:\n",
    "\n",
    "There is NO default RPC flow like that.\n",
    "\n",
    "Typical Avro flow:\n",
    "\n",
    "```\n",
    "producer ‚Üí serialize to Avro ‚Üí send to Kafka ‚Üí consumer\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```\n",
    "application ‚Üí write Avro file ‚Üí Hadoop processes file\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```\n",
    "HTTP API ‚Üí send Avro bytes ‚Üí backend parses Avro\n",
    "```\n",
    "\n",
    "So:\n",
    "\n",
    "### ‚úî Conceptually similar to Protobuf and Thrift\n",
    "\n",
    "(because all do schema-based serialization)\n",
    "\n",
    "### ‚ùå But operationally very different\n",
    "\n",
    "(because Avro is built for distributed data storage, not RPC calls)\n",
    "\n",
    "---\n",
    "\n",
    "# 5Ô∏è‚É£ Summary (One-Shot Explanation)\n",
    "\n",
    "**Apache Avro** is a serialization system designed for big data pipelines.\n",
    "It stores schemas in JSON, can embed schemas inside the data, supports dynamic typing, and is the default for Kafka/Hadoop. Unlike Protobuf and Thrift, Avro is not primarily used for RPC, and you don‚Äôt need generated code ‚Äî it serializes raw maps/dicts. It excels at schema evolution and distributed processing.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Avro\n",
    "\n",
    "Avro has the following types:\n",
    "\n",
    "- null: no value\n",
    "- boolean: a binary value\n",
    "- int: 32-bit signed integer\n",
    "- long: 64-bit signed integer\n",
    "- float: single precision (32-bit) IEEE 754 floating-point number\n",
    "- double: double precision (64-bit) IEEE 754 floating-point number\n",
    "- bytes: sequence of 8-bit unsigned bytes\n",
    "- string: Unicode character sequence\n",
    "- record: ordered collection of named fields\n",
    "- enum: enumeration of string values\n",
    "- array: ordered collection of values\n",
    "- map: collection of key-value pairs\n",
    "- union: ordered list of values\n",
    "\n",
    "It has two schema languages: one (`Avro IDL`) intended for human editing, and one (based on JSON) that is more easily machine-readable.\n",
    "\n",
    "### Encoding\n",
    "\n",
    "We can encode the previous example record in IDL using the following schema in the `.avsc` file:\n",
    "\n",
    "```avro\n",
    "record Person {\n",
    "  string userName;\n",
    "  union { null, long } favoriteNumber = null;\n",
    "  array<string> interests;\n",
    "}\n",
    "```\n",
    "\n",
    "The equivalent JSON representation of that schema is as follows:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"type\": \"record\",\n",
    "  \"name\": \"Person\",\n",
    "  \"fields\": [\n",
    "    { \"name\": \"userName\", \"type\": \"string\" },\n",
    "    { \"name\": \"favoriteNumber\", \"type\": [\"null\", \"long\"], \"default\": null },\n",
    "    { \"name\": \"interests\", \"type\": { \"type\": \"array\", \"items\": \"string\" } }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "The data encoded with this schema looks like this:\n",
    "![avro](../assets/avro.png)\n",
    "\n",
    "First and foremost, it's important to note that the schema lacks tag numbers. When we encode our sample record using this schema, the resulting Avro binary encoding is impressively compact, spanning just _32 bytes_‚Äîthe most space-efficient among all the encodings we've observed.\n",
    "\n",
    "Examining the byte sequence, one can readily discern the _absence of field identifiers or datatype markers_. The encoding solely comprises concatenated values. For instance, a string is represented by a length prefix followed by UTF-8 bytes, but there are no explicit indicators within the encoded data to specify that it is, indeed, a string. In fact, it could be interpreted as an integer or any other data type altogether. Similarly, an integer is encoded using a variable-length encoding.\n",
    "\n",
    "To correctly parse the binary data, you must traverse the fields in the order they appear in the schema and _refer to the schema_ itself to ascertain the datatype of each field. Consequently, the binary data can only be accurately decoded if the code reading the data employs the exact same schema as the code that wrote the data. Any deviation or mismatch in the schema between the reader and the writer would result in incorrectly decoded data.\n",
    "\n",
    "With Avro, data encoding and decoding are based on two schemas: the `writer's schema` used during data encoding and the `reader's schema` employed during data decoding. These schemas do not necessarily have to be identical but should be compatible. When decoding data, the Avro library compares the writer's and reader's schemas, resolving any discrepancies between them.\n",
    "\n",
    "The Avro specification ensures that fields in different orders between the writer's and reader's schemas pose no issues during resolution since schema matching occurs based on field names. If the reader's schema lacks a field present in the writer's schema, it is simply ignored. Conversely, if the reader's schema expects a field that the writer's schema does not contain, the missing field is filled in with a default value declared in the reader's schema. This allows for flexible schema evolution while maintaining data compatibility.\n",
    "\n",
    "### Reading (Decoding) a File\n",
    "\n",
    "Instead of demonstrating RPC, let's look at how to decode data from a file from a real-world dataset. We have a genomic variation data of 1000 samples from the [OpenCGA](http://docs.opencb.org/display/opencga/Welcome+to+OpenCGA) project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastavro\n",
    "import copy\n",
    "import json\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "copy library <- python's built-in module for copying objects\n",
    "pprint<- Pretty-Print module. Makes nested dicts and JSON objects easier to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/1k.variants.avro', 'rb') as f:\n",
    "    reader = fastavro.reader(f)\n",
    "    genomic_var_1k = [sample for sample in reader]\n",
    "    metadata = copy.deepcopy(reader.metadata)\n",
    "    writer_schema = copy.deepcopy(reader.writer_schema)\n",
    "    schema_from_file = json.loads(metadata['avro.schema'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ 1. with open('../data/1k.variants.avro', 'rb') as f:\n",
    "\n",
    "    open the Avro file in inary mode\n",
    "    Avro files are always binary containers, not plain JSON\n",
    "\n",
    "‚úÖ 2. reader = fastavro.reader(f)\n",
    "\n",
    "    creates the Avro file reader\n",
    "    Does the following automatically:\n",
    "        .reads Avro file header\n",
    "        . extracts embedded writer schema\n",
    "        . extracts metadata\n",
    "        . perpares to decode binary blocks\n",
    "\n",
    "‚úÖ 3. genomic_var_1k = [sample for sample in reader]\n",
    "\n",
    "    Iterates through every record in the Avro file\n",
    "    Each item returned is a Python dictionary that matches the schema (No class generation needed)\n",
    "\n",
    "‚úÖ 4. metadata = copy.deepcopy(reader.metadata)\n",
    "\n",
    "    Avro files store metadata in the header\n",
    "\n",
    "‚úÖ 5. writer_schema = copy.deepcopy(reader.writer_schema)\n",
    "\n",
    "    This retrieves the actual schema used when the file was written.\n",
    "\n",
    "‚úÖ 6. schema_from_file = json.loads(metadata['avro.schema'])\n",
    "\n",
    "    Takes the raw JSON string stored in metadata and converts it into a Python dict.\n",
    "    This will be identical to writer_schema\n",
    "\n",
    "üî• Complete Summary Table (Easy View)\n",
    "\n",
    "| Variable           | Meaning                              | Why Needed                          |\n",
    "| ------------------ | ------------------------------------ | ----------------------------------- |\n",
    "| `genomic_var_1k`   | All decoded records in the Avro file | Data itself                         |\n",
    "| `metadata`         | Header metadata                      | Contains raw schema & codec         |\n",
    "| `writer_schema`    | Parsed schema used to write file     | Needed for re-writing or validating |\n",
    "| `schema_from_file` | Schema reconstructed from metadata   | Usually for debugging or display    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(genomic_var_1k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pprint(writer_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(schema_from_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(genomic_var_1k[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in schema_from_file[\"fields\"]:\n",
    "    print(f[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bde]",
   "language": "python",
   "name": "conda-env-bde-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
