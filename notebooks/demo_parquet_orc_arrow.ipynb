{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Encoding, Decoding and Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All three (Parquet , ORC and Arrow) deal with strcurtured data.\n",
    "\n",
    "---\n",
    "\n",
    "# üü¶ **1. Apache Parquet**\n",
    "\n",
    "### üìå *A columnar **storage file format**.*\n",
    "\n",
    "Parquet is used to **store** data on disk (or cloud storage like S3) in a way that is:\n",
    "\n",
    "* Columnar (column by column)\n",
    "* Highly compressed (Snappy, ZSTD, etc.)\n",
    "* Optimized for analytics (fast filtering & aggregations)\n",
    "* Splittable (for parallel reading in Spark, Hive, DuckDB, etc.)\n",
    "\n",
    "### üìå Where you see Parquet\n",
    "\n",
    "‚úî Data lakes (S3, GCS, HDFS)\n",
    "‚úî Spark / Databricks\n",
    "‚úî Snowflake, BigQuery, Redshift Spectrum\n",
    "‚úî Pandas & Polars\n",
    "\n",
    "### üìå Example usage\n",
    "\n",
    "Save a dataset to a Parquet file:\n",
    "\n",
    "```python\n",
    "df.to_parquet(\"users.parquet\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üü© **2. ORC (Optimized Row Columnar)**\n",
    "\n",
    "### üìå *Another columnar **storage file format**, similar to Parquet.*\n",
    "\n",
    "Parquet and ORC are competitors.\n",
    "Both are used for big-data analytics.\n",
    "\n",
    "ORC is:\n",
    "\n",
    "* Very compact (often smaller than Parquet)\n",
    "* Highly optimized for **Hadoop & Hive**\n",
    "* Fast for certain workloads\n",
    "\n",
    "### üìå Where ORC is used\n",
    "\n",
    "‚úî Hive\n",
    "‚úî Presto / Trino\n",
    "‚úî Big Hadoop installations\n",
    "‚úî Some Spark setups (less common now)\n",
    "\n",
    "### üìå Example usage (Spark)\n",
    "\n",
    "```python\n",
    "df.write.orc(\"example.orc\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üü• **3. Apache Arrow**\n",
    "\n",
    "### üìå *NOT a file format ‚Äî it is an **in-memory data format**.*\n",
    "\n",
    "This is the most important distinction:\n",
    "\n",
    "‚ö†Ô∏è **Arrow is NOT a storage file format like Parquet or ORC.**\n",
    "It does not store data on disk.\n",
    "\n",
    "Arrow is a **way to store data in RAM** so that analytics tools can share memory *without copying data*.\n",
    "\n",
    "Arrow stores data in **columnar, contiguous memory buffers** ‚Äî this makes operations extremely fast.\n",
    "\n",
    "### üìå Why Arrow exists\n",
    "\n",
    "Before Arrow:\n",
    "\n",
    "* Pandas ‚Üí copies data to NumPy\n",
    "* Spark ‚Üí copies data when converting to Pandas\n",
    "* R, Python, Java, C++ all use different memory layouts\n",
    "* Serialization overhead everywhere\n",
    "\n",
    "Arrow solves this by providing *one universal memory layout*.\n",
    "\n",
    "### üìå Benefits\n",
    "\n",
    "‚úî Zero-copy data interchange (Python ‚Üî Spark ‚Üî R ‚Üî C++ ‚Ä¶)\n",
    "‚úî Faster analytics\n",
    "‚úî Backbone of many modern engines: Pandas 2.0, Polars, DuckDB\n",
    "\n",
    "### üìå Where you see Arrow\n",
    "\n",
    "‚úî Pandas 2.0 default engine\n",
    "‚úî Spark uses Arrow for Pandas UDFs\n",
    "‚úî Polars native engine\n",
    "‚úî PyArrow library\n",
    "‚úî DuckDB integration\n",
    "\n",
    "### Example: Arrow Table\n",
    "\n",
    "```python\n",
    "import pyarrow as pa\n",
    "\n",
    "table = pa.table({\"a\": [1,2,3], \"b\": [\"x\",\"y\",\"z\"]})\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üß† So how do Parquet, ORC, and Arrow relate?\n",
    "\n",
    "They are **complementary**, not competing.\n",
    "\n",
    "### ‚úî Parquet ‚Üí columnar file on disk\n",
    "\n",
    "### ‚úî ORC ‚Üí columnar file on disk\n",
    "\n",
    "### ‚úî Arrow ‚Üí columnar format in memory\n",
    "\n",
    "Think of it like this:\n",
    "\n",
    "| Layer                    | Technology                    |\n",
    "| ------------------------ | ----------------------------- |\n",
    "| **In-memory processing** | ‚ûú Apache Arrow                |\n",
    "| **On-disk storage**      | ‚ûú Parquet, ORC                |\n",
    "| **Higher-level tools**   | Pandas, Spark, Polars, DuckDB |\n",
    "\n",
    "---\n",
    "\n",
    "# üßä Simple Analogy (super easy to visualize)\n",
    "\n",
    "### **Arrow = Serving food on plates (ready to eat)**\n",
    "\n",
    "Everything laid out nicely, ready for quick access.\n",
    "\n",
    "### **Parquet / ORC = Food containers stored in the fridge**\n",
    "\n",
    "Compressed for storage, not for immediate serving.\n",
    "\n",
    "When you load a Parquet file, many tools convert it into Arrow memory format automatically.\n",
    "\n",
    "---\n",
    "\n",
    "# üìù One-sentence definitions\n",
    "\n",
    "### **Parquet:**\n",
    "\n",
    "*A compressed, columnar file format optimized for analytical queries.*\n",
    "\n",
    "### **ORC:**\n",
    "\n",
    "*A Hadoop-optimized columnar file format similar to Parquet, often with better compression.*\n",
    "\n",
    "### **Apache Arrow:**\n",
    "\n",
    "*An in-memory, columnar data format designed for ultra-fast analytics and zero-copy data sharing across systems.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avro, Thrift, Protobuf VS Parquet, ORC, Arrow\n",
    "\n",
    "---\n",
    "\n",
    "# ‚úÖ **1. Avro, Thrift, Protobuf ‚Üí *Serialization formats***\n",
    "\n",
    "These are **encode/decode systems** that convert **in-memory objects ‚Üí binary bytes ‚Üí back to objects**.\n",
    "\n",
    "They are used for:\n",
    "\n",
    "* RPC frameworks (remote calls)\n",
    "* Message passing\n",
    "* Kafka messages\n",
    "* Network communication\n",
    "* Storing data in binary form (but not optimized for analytics)\n",
    "\n",
    "### ‚úî They define how to **serialize** data\n",
    "\n",
    "They are *not* optimized for analytical querying.\n",
    "\n",
    "### ‚úî They are schema-based\n",
    "\n",
    "* Protobuf = .proto\n",
    "* Thrift = .thrift\n",
    "* Avro = .avsc (JSON schema)\n",
    "\n",
    "### ‚úî Output is usually **binary**\n",
    "\n",
    "Not human-readable.\n",
    "\n",
    "---\n",
    "\n",
    "# üü• **2. Parquet, ORC, Arrow ‚Üí *Columnar storage formats***\n",
    "\n",
    "These are **data storage / representation formats**, not RPC serialization formats.\n",
    "\n",
    "### ‚úî Used for analytics\n",
    "\n",
    "* Data lakes (S3/GCS/Azure/HDFS)\n",
    "* Spark\n",
    "* BigQuery / Snowflake\n",
    "* DuckDB / Polars\n",
    "\n",
    "### ‚úî Columnar = optimized for:\n",
    "\n",
    "* Filtering\n",
    "* Aggregation\n",
    "* Scanning billions of rows\n",
    "* Compression\n",
    "* Vectorized execution\n",
    "\n",
    "### ‚úî NOT used for RPC\n",
    "\n",
    "You do not send Parquet or ORC through network RPC systems.\n",
    "\n",
    "---\n",
    "\n",
    "# üü© **So the correct categorization:**\n",
    "\n",
    "## üü° **Serialization / Transport / RPC Formats**\n",
    "\n",
    "Used for messaging, RPC, Kafka, microservices:\n",
    "\n",
    "| Format       | Purpose                      |\n",
    "| ------------ | ---------------------------- |\n",
    "| **Protobuf** | gRPC, internal communication |\n",
    "| **Thrift**   | services + serialization     |\n",
    "| **Avro**     | Kafka + schema evolution     |\n",
    "\n",
    "‚û° **Focus:** encode/decode speed, schema evolution.\n",
    "\n",
    "---\n",
    "\n",
    "## üîµ **Analytical Storage Formats (File formats for data lakes)**\n",
    "\n",
    "| Format      | Purpose                         |\n",
    "| ----------- | ------------------------------- |\n",
    "| **Parquet** | analytical columnar storage     |\n",
    "| **ORC**     | Hive-optimized columnar storage |\n",
    "| **Arrow**   | *in-memory* columnar format     |\n",
    "\n",
    "‚û° **Focus:** compression, scan speed, analytics.\n",
    "\n",
    "---\n",
    "\n",
    "# üß† **Key Distinction (Super Simple)**\n",
    "\n",
    "### **Protobuf / Avro / Thrift**\n",
    "\n",
    "* For communication\n",
    "* Small messages\n",
    "* Network RPC\n",
    "* Schema-based serialization\n",
    "* Optimized for encode/decode speed\n",
    "* Used inside programs\n",
    "\n",
    "### **Parquet / ORC**\n",
    "\n",
    "* For storage\n",
    "* Large datasets\n",
    "* Analytics on GB ‚Üí TB data\n",
    "* Columnar\n",
    "* Compressed\n",
    "* Optimized for reading only needed columns\n",
    "\n",
    "### **Arrow**\n",
    "\n",
    "* In-memory representation\n",
    "* Ultra-fast processing\n",
    "* Zero-copy between languages (Python ‚Üî C++ ‚Üî R ‚Üî Rust ‚Üî Java)\n",
    "* Often used *after* you load Parquet\n",
    "\n",
    "---\n",
    "\n",
    "# üßä Real-life analogy (easy)\n",
    "\n",
    "### Serialization formats (Protobuf/Avro/Thrift)\n",
    "\n",
    "‚û° \"Shipping boxes\"\n",
    "Designed to move data efficiently from point A ‚Üí B.\n",
    "\n",
    "### Parquet/ORC\n",
    "\n",
    "‚û° \"Warehouse storage shelves\"\n",
    "Designed to store huge amounts of data efficiently and retrieve only what you need.\n",
    "\n",
    "### Arrow\n",
    "\n",
    "‚û° \"Items placed on working tables\"\n",
    "Optimized for immediate processing, no storage.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Parquet, ORC and Arrow\n",
    "\n",
    "We can easily read (decode) and write (encode) data from and to Parquet, ORC and Arrow files interchangeably. The `pyarrow` library allows us to read a Parquet or ORC file into a `pyarrow.Table` object, which is a columnar data structure that can be converted to a Pandas DataFrame. We can also write a `pyarrow.Table` to a Parquet or ORC file.\n",
    "\n",
    "Parquet has the following types:\n",
    "\n",
    "- boolean: 1 bit boolean\n",
    "- int32: 32 bit signed ints\n",
    "- int64: 64 bit signed ints\n",
    "- int96: 96 bit signed ints\n",
    "- float: IEEE 32-bit floating point values\n",
    "- double: IEEE 64-bit floating point values\n",
    "- byte_array: arbitrarily long byte arrays\n",
    "- fixed_len_byte_array: fixed length byte arrays\n",
    "- string: UTF-8 encoded strings\n",
    "- enum: enumeration of strings\n",
    "- temporal: a logical date type\n",
    "\n",
    "ORC has the following types:\n",
    "\n",
    "- boolean: 1 bit boolean\n",
    "- tinyint: 8 bit signed ints\n",
    "- smallint: 16 bit signed ints\n",
    "- int: 32 bit signed ints\n",
    "- bigint: 64 bit signed ints\n",
    "- float: IEEE 32-bit floating point values\n",
    "- double: IEEE 64-bit floating point values\n",
    "- string: UTF-8 encoded strings\n",
    "- char: ASCII strings\n",
    "- varchar: UTF-8 strings\n",
    "- binary: byte arrays\n",
    "- timestamp: a logical date type\n",
    "- date: a logical date type\n",
    "- decimal: arbitrary precision decimals\n",
    "- list: an ordered collection of objects\n",
    "- map: a collection of key-value pairs\n",
    "- struct: an ordered collection of named fields\n",
    "- union: a list of types\n",
    "\n",
    "![overview-diagram](../assets/diagram-2.png)\n",
    "\n",
    "### Reading (Decoding) and Writing (Encoding) a Parquet File\n",
    "\n",
    "Let's look at how to decode and encode a Parquet file with mock customers data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# **How it works:**\n",
    "\n",
    "1. **Parquet / ORC**\n",
    "\n",
    "* Are **on-disk, columnar storage formats**.\n",
    "* Highly compressed, optimized for analytics queries and storage efficiency.\n",
    "* Ideal for ‚Äúdata at rest‚Äù in a data lake or warehouse.\n",
    "\n",
    "2. **When you want to analyze the data**\n",
    "\n",
    "* Libraries like **PyArrow, Pandas 2.0, Spark, DuckDB, Polars** **read the Parquet/ORC file into memory**.\n",
    "* Internally, they convert it into **Apache Arrow‚Äôs in-memory columnar format**.\n",
    "\n",
    "3. **In-memory (Arrow Table)**\n",
    "\n",
    "* Arrow holds data in **contiguous, columnar buffers**.\n",
    "* Extremely fast for analytics and vectorized operations.\n",
    "* Multiple languages/tools can share Arrow memory without copying.\n",
    "\n",
    "4. **Display / work in Pandas**\n",
    "\n",
    "* Arrow Table ‚Üí Pandas DataFrame (or Polars DataFrame).\n",
    "* Now you can use Python for analysis, visualization, ML, etc.\n",
    "\n",
    "---\n",
    "\n",
    "# **Data Flow Visualization**\n",
    "\n",
    "```\n",
    "Storage on disk:    Parquet / ORC files\n",
    "            ‚Üì   (read)\n",
    "In-memory format:   Apache Arrow Table\n",
    "            ‚Üì   (convert to Python objects)\n",
    "User interface:     Pandas DataFrame / Polars DataFrame\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# **Key points**\n",
    "\n",
    "* **Parquet/ORC** ‚Üí storage\n",
    "* **Arrow** ‚Üí in-memory processing\n",
    "* **Pandas** ‚Üí user-facing analytics library\n",
    "* You **never manipulate Parquet/ORC files directly** in Pandas; they are first loaded into Arrow buffers.\n",
    "* This design allows **fast analytics on very large datasets** without decompressing everything unnecessarily.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyarrow <- Python library for Apache Arrow\n",
    "    Provides:\n",
    "        .In-memory columnar data structures (pa.Table, pa.Array, pa.RecordBatch)\n",
    "        .Efficient memory representation for analytics and zero-copy sharing\n",
    "        .Conversion between Pandas DataFrames <-> Arrow Tables\n",
    "        .Varios data types: string, int, float, timestamp, nested types etc\n",
    "\n",
    "pyarrow.parquet <- module for working with Parqurt files\n",
    "    Provides:\n",
    "        .Read Parquet -> Arrow Table (pq.read_table)\n",
    "        .Rad only metdata (pq.read_metdata)\n",
    "        .Write Arrow Table -> Parquet file (pq.write_table)\n",
    "        .Read only specific columns or row groups for efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How they work together\n",
    "\n",
    "| Module                 | Role                                                           |\n",
    "| ---------------------- | -------------------------------------------------------------- |\n",
    "| `pyarrow` (pa)         | Provides in-memory columnar structures (Arrow Tables)          |\n",
    "| `pyarrow.parquet` (pq) | Reads/writes **disk-based Parquet files** to/from Arrow Tables |\n",
    "\n",
    "\n",
    "Typical Workflow:\n",
    "1. Parquet file on disk ‚Üí pq.read_table() ‚Üí Arrow Table (pa.Table)\n",
    "2. Arrow Table ‚Üí to_pandas() ‚Üí Pandas DataFrame for analysis\n",
    "3. After analysis ‚Üí pq.write_table() ‚Üí Parquet file to store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pq.read_table('../data/userdata1.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell block above - Reading a Parquet file into memory using Apache Arrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pq.read_metadata('../data/userdata1.parquet')\n",
    "\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read_metadata() <-readys only the metadata of a Parquet file, without loading the full dataset into memory\n",
    "\n",
    "Why use read_metadata\n",
    "Fast: avoids loading full data\n",
    "Inspect schema & stats before reading\n",
    "Plan queries / filtering efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.row_group(0).column(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell block:\n",
    "\n",
    "metadata.row_group(0)\n",
    "    . Selects the first row group (0-indexed)\n",
    "    . Returns a RowGroupMetaData object\n",
    "\n",
    ".column(10)\n",
    "    . Selects the 11th colum (0-indexed) in that row group\n",
    "    . Returns a ColumnChunkMetaData Object\n",
    "    . ColumnMetaData gives information about that column in the row group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the first 3 rows of the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.take([0,1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert a Table to a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = table.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can convert the DataFrame back to a Table (note we're using the method from `pa` which is pyarrow):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_table = pa.Table.from_pandas(df)\n",
    "\n",
    "new_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can write the table back to a Parquet file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq.write_table(new_table, \"../data/userdata2.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. How many males and females are there?\n",
    ">\n",
    "> 2. What is the average salary for customers from China?\n",
    ">\n",
    "> 3. Create a new column `full_name` which combines `first_name` and `last_name` with a space in between in the dataframe. Then convert it back to a new Table and write it to a Parquet file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading (Decoding) and Writing (Encoding) an ORC File\n",
    "\n",
    "Let's look at how to decode and encode an ORC file with mock data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "from pyarrow import orc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table2 = orc.read_table('../data/userdata1.1.orc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = table2.to_pandas()\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can write the table back to an ORC file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orc.write_table(table2, \"../data/file2.orc\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bde]",
   "language": "python",
   "name": "conda-env-bde-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
